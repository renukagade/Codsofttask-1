# -*- coding: utf-8 -*-
"""codesofttask-1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Luj7nhAfnLb-Q-HT21mxWWKK8LslmkhP
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn import tree
import math
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

data=pd.read_csv("/content/Titanic-Dataset.csv")

data

data.loc[25]

data.columns

features = data.columns[1:]
features

data.shape

data.info()

data.describe()

data.head()

data.tail(100)

print(data.isnull().sum())

selected_columns=data[['Name','Sex']]
print(selected_columns)

filtered_data=data[data['Age']>25]
print(filtered_data)

sorted_data=data.sort_values(by='Age', ascending=False)
print(sorted_data)

data.select_dtypes("object").nunique()

data.drop('Cabin', axis=1,inplace=True)

data.columns

sns.countplot(x= data["Survived"] , hue = data["Pclass"])

num_samples = 1000
gender = np.random.choice(['Male', 'Female'], num_samples)
male_count = np.count_nonzero(gender == 'Male')
female_count = num_samples - male_count

# Plotting the pie chart
plt.figure(figsize=(8, 6))
plt.pie([male_count, female_count], labels=['Male', 'Female'], autopct='%.1f%%', colors=['lightblue', 'pink'])
plt.title('Male & Female Distribution')
plt.legend()
plt.show()

data = np.random.randn(1000)
plt.hist(data,bins=30,color='skyblue',edgecolor='black')
plt.title('Histogram')
plt.grid(True)
plt.show()

test = data.drop(['PassengerId', 'Name', 'Ticket'], axis=1)
test

label_encoder = LabelEncoder()

test['Sex'] = label_encoder.fit_transform(test['Sex'])
test['Embarked'] = label_encoder.fit_transform(test['Embarked'])
test

data.Embarked.fillna(data.Embarked.mode()[0],inplace=True)

data.info()

def data_info(data):
    cols=[]
    unique_val=[]
    n_uniques=[]
    dtypes=[]
    nulls=[]
    for col in data.columns:
        cols.append(col)
        dtypes.append(data[col].dtype)
        n_uniques.append(data[col].nunique())
        unique_val.append(data[col].unique())
        nulls.append(data[col].isna().sum())
    return pd.DataFrame({"Col":cols,"dtype":dtypes,"n_uniques":n_uniques,"unique Values":unique_val,"Nulls":nulls})

data_info(data)

columns=['Age','Fare']
for col in columns:
  data[col].fillna(data[col].median(),inplace=True)
data['Cabin'].fillna('Unknown', inplace=True)

data_info(data)

data.Survived.value_counts()

survived_counts = data['Survived'].value_counts()
fig_surv_perc = px.pie(data, names= survived_counts.index,  values = survived_counts.values, title=f'Distribution of Survived', hole=0.2, color_discrete_sequence=px.colors.sequential.Viridis)
fig_surv_perc.update_traces(textinfo='percent+label')
fig_surv_perc.update_layout(legend_title_text='Categories:', legend=dict(orientation="h", yanchor="bottom", y=1.02))
fig_surv_perc.show()

target = data.Survived

data = data.drop(["Name", "Ticket", "Cabin", "Survived"], axis=1)
# df.isnull().any()
data.isna().sum()

age_median = data.Age.median()
data.Age = data.Age.fillna(math.floor(age_median))
data.isna().sum()

from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

# Create the ColumnTransformer
ct = ColumnTransformer(transformers=[('encoder',
                                      OneHotEncoder(handle_unknown='ignore'),
                                      ['Sex', 'Embarked'])
                                    ],
                       remainder='passthrough',
                       sparse_threshold=0)

# Fit and transform the data
data_encoded = ct.fit_transform(data)

# Convert the sparse matrix back to a DataFrame
new_df = pd.DataFrame(data_encoded, columns=ct.get_feature_names_out())
new_df = new_df.drop("encoder__Embarked_nan", axis=1)

x_train,x_test,y_train,y_test = train_test_split(new_df, target, test_size=0.2)

model = LogisticRegression()
model.fit(x_train, y_train)
model.predict(x_test)

model.score(x_test, y_test)